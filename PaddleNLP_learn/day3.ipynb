{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业\n",
    "\n",
    "更换数据集MSRA和ERNIE-Gram或BERT等预训练模型。\n",
    "\n",
    "- 数据集：\n",
    "`train_ds, test_ds = load_dataset(\"msra_ner\", splits=[\"train\", \"test\"])`\n",
    "- 模型：\n",
    "\t将`from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification`换成相应的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 使用PaddleNLP语义预训练模型ERNIE完成快递单信息抽取\n",
    "\n",
    "\n",
    "**注意**\n",
    "\n",
    "本项目代码需要使用GPU环境来运行:\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/767f625548714f03b105b6ccb3aa78df9080e38d329e445380f505ddec6c7042\" width=\"40%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "命名实体识别是NLP中一项非常基础的任务，是信息提取、问答系统、句法分析、机器翻译等众多NLP任务的重要基础工具。命名实体识别的准确度，决定了下游任务的效果，是NLP中的一个基础问题。在NER任务提供了两种解决方案，一类LSTM/GRU + CRF，通过RNN类的模型来抽取底层文本的信息，而CRF(条件随机场)模型来学习底层Token之间的联系；另外一类是通过预训练模型，例如ERNIE，BERT模型，直接来预测Token的标签信息。\n",
    "\n",
    "本项目将演示如何使用PaddleNLP语义预训练模型ERNIE完成从快递单中抽取姓名、电话、省、市、区、详细地址等内容，形成结构化信息。辅助物流行业从业者进行有效信息的提取，从而降低客户填单的成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在2017年之前，工业界和学术界对文本处理依赖于序列模型[Recurrent Neural Network (RNN)](https://baike.baidu.com/item/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/23199490?fromtitle=RNN&fromid=5707183&fr=aladdin).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png\" width=\"40%\" height=\"30%\"> <br />\n",
    "</p><br><center>图1：RNN示意图</center></br>\n",
    "\n",
    "[基于BiGRU+CRF的快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)项目介绍了如何使用序列模型完成快递单信息抽取任务。\n",
    "<br>\n",
    "\n",
    "近年来随着深度学习的发展，模型参数的数量飞速增长。为了训练这些参数，需要更大的数据集来避免过拟合。然而，对于大部分NLP任务来说，构建大规模的标注数据集非常困难（成本过高），特别是对于句法和语义相关的任务。相比之下，大规模的未标注语料库的构建则相对容易。为了利用这些数据，我们可以先从其中学习到一个好的表示，再将这些表示应用到其他任务中。最近的研究表明，基于大规模未标注语料库的预训练模型（Pretrained Models, PTM) 在NLP任务上取得了很好的表现。\n",
    "\n",
    "近年来，大量的研究表明基于大型语料库的预训练模型（Pretrained Models, PTM）可以学习通用的语言表示，有利于下游NLP任务，同时能够避免从零开始训练模型。随着计算能力的不断提高，深度模型的出现（即 Transformer）和训练技巧的增强使得 PTM 不断发展，由浅变深。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/327f44ff3ed24493adca5ddc4dc24bf61eebe67c84a6492f872406f464fde91e\" width=\"60%\" height=\"50%\"> <br />\n",
    "</p><br><center>图2：预训练模型一览，图片来源于：https://github.com/thunlp/PLMpapers</center></br>\n",
    "                                                                                                                             \n",
    "本示例展示了以ERNIE([Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223))为代表的预训练模型如何Finetune完成序列标注任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐**\n",
    "\n",
    "开源不易，希望大家多多支持~ \n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a0e8ca7743ea4fe9aa741682a63e767f8c48dc55981f4e44a40e0e00d3ab369e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AI Studio平台后续会默认安装PaddleNLP，在此之前可使用如下命令安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_a\tlabel\r\n",
      "黑\u0002龙\u0002江\u0002省\u0002双\u0002鸭\u0002山\u0002市\u0002尖\u0002山\u0002区\u0002八\u0002马\u0002路\u0002与\u0002东\u0002平\u0002行\u0002路\u0002交\u0002叉\u0002口\u0002北\u00024\u00020\u0002米\u0002韦\u0002业\u0002涛\u00021\u00028\u00026\u00020\u00020\u00020\u00020\u00029\u00021\u00027\u00022\tA1-B\u0002A1-I\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002P-B\u0002P-I\u0002P-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\r\n",
      "广\u0002西\u0002壮\u0002族\u0002自\u0002治\u0002区\u0002桂\u0002林\u0002市\u0002雁\u0002山\u0002区\u0002雁\u0002山\u0002镇\u0002西\u0002龙\u0002村\u0002老\u0002年\u0002活\u0002动\u0002中\u0002心\u00021\u00027\u00026\u00021\u00020\u00023\u00024\u00028\u00028\u00028\u00028\u0002羊\u0002卓\u0002卫\tA1-B\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\u0002P-I\r\n",
      "1\u00025\u00026\u00025\u00022\u00028\u00026\u00024\u00025\u00026\u00021\u0002河\u0002南\u0002省\u0002开\u0002封\u0002市\u0002顺\u0002河\u0002回\u0002族\u0002区\u0002顺\u0002河\u0002区\u0002公\u0002园\u0002路\u00023\u00022\u0002号\u0002赵\u0002本\u0002山\tT-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002A1-B\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002P-B\u0002P-I\u0002P-I\r\n",
      "河\u0002北\u0002省\u0002唐\u0002山\u0002市\u0002玉\u0002田\u0002县\u0002无\u0002终\u0002大\u0002街\u00021\u00025\u00029\u0002号\u00021\u00028\u00026\u00021\u00024\u00022\u00025\u00023\u00020\u00025\u00028\u0002尚\u0002汉\u0002生\tA1-B\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\u0002P-I\r\n"
     ]
    }
   ],
   "source": [
    "# 下载并解压数据集\n",
    "from paddle.utils.download import get_path_from_url\n",
    "URL = \"https://paddlenlp.bj.bcebos.com/paddlenlp/datasets/waybill.tar.gz\"\n",
    "get_path_from_url(URL, \"./\")\n",
    "\n",
    "# 查看预测的数据\n",
    "!head -n 5 data/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这里对于utils.py，需要参考[https://github.com/PaddlePaddle/PaddleNLP/blob/develop/education/day03.md](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/education/day03.md) \n",
    "和[https://github.com/PaddlePaddle/PaddleNLP/blob/develop/examples/information_extraction/msra_ner/predict.py](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/examples/information_extraction/msra_ner/predict.py)\n",
    "进行修改\n",
    "\n",
    "主要原因是两个数据集的标注形式的不同\n",
    "\n",
    "微软的数据集采用了'BIO'在前的标注方式\n",
    "\n",
    "```\n",
    "不\\002久\\002前\\002，\\002中\\002国\\002共\\002产\\002党\\002召\\002开\\002了\\002举\\002世\\002瞩\\002目\\002的\\002第\\002十\\002五\\002次\\002全\\002国\\002代\\002表\\002大\\002会\\002。    O\\002O\\002O\\002O\\002B-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002B-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002O\n",
    "这\\002次\\002代\\002表\\002大\\002会\\002是\\002在\\002中\\002国\\002改\\002革\\002开\\002放\\002和\\002社\\002会\\002主\\002义\\002现\\002代\\002化\\002建\\002设\\002发\\002展\\002的\\002关\\002键\\002时\\002刻\\002召\\002开\\002的\\002历\\002史\\002性\\002会\\002议\\002。    O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002B-LOC\\002I-LOC\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\n",
    "```\n",
    "需要修改parse_decondes函数\n",
    "```\n",
    "\n",
    "def parse_decodes(ds, decodes, lens, label_vocab):\n",
    "    decodes = [x for batch in decodes for x in batch]\n",
    "    lens = [x for batch in lens for x in batch]\n",
    "    id_label = dict(zip(label_vocab.values(), label_vocab.keys()))\n",
    "\n",
    "    outputs = []\n",
    "    for idx, end in enumerate(lens):\n",
    "        sent = ds.data[idx]['tokens'][:end]\n",
    "        tags = [id_label[x] for x in decodes[idx][1:end]]\n",
    "        sent_out = []\n",
    "        tags_out = []\n",
    "        words = \"\"\n",
    "        for s, t in zip(sent, tags):\n",
    "            if t.startswith('B-') or t == 'O':\n",
    "                if len(words):\n",
    "                    sent_out.append(words)\n",
    "                if t.startswith('B-'):\n",
    "                    tags_out.append(t.split('-')[1])\n",
    "                else:\n",
    "                    tags_out.append(t)\n",
    "                words = s\n",
    "            else:\n",
    "                words += s\n",
    "        if len(sent_out) < len(tags_out):\n",
    "            sent_out.append(words)\n",
    "        outputs.append(''.join(\n",
    "            [str((s, t)) for s, t in zip(sent_out, tags_out)]))\n",
    "    return outputs\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import paddle\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "from utils import convert_example, evaluate, predict, load_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 加载自定义数据集\n",
    "\n",
    "推荐使用MapDataset()自定义数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def load_dataset(datafiles):\n",
    "#     def read(data_path):\n",
    "#         with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "#             next(fp)  # Skip header\n",
    "#             for line in fp.readlines():\n",
    "#                 words, labels = line.strip('\\n').split('\\t')\n",
    "#                 words = words.split('\\002')\n",
    "#                 labels = labels.split('\\002')\n",
    "#                 yield words, labels\n",
    "\n",
    "#     if isinstance(datafiles, str):\n",
    "#         return MapDataset(list(read(datafiles)))\n",
    "#     elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "#         return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "\n",
    "# # Create dataset, tokenizer and dataloader.\n",
    "# train_ds, dev_ds, test_ds = load_dataset(datafiles=(\n",
    "#         './data/train.txt', './data/dev.txt', './data/test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这里加载一下微软的数据集：由于预置的数据集只有训练和测试集，所以这里把测试集当作验证集来用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "# train_ds, test_ds = load_dataset(\"msra_ner\", splits=[\"train\",\"test\"],lazy=False)\r\n",
    "train_ds, dev_ds, test_ds = load_dataset(\r\n",
    "        'msra_ner', splits=('train', 'test', 'test'), lazy=False)\r\n",
    "\r\n",
    "label_vocab = {label:label_id for label_id, label in enumerate(train_ds.label_list)}\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], 'labels': [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]}\n",
      "{'tokens': ['藏', '书', '本', '来', '就', '是', '所', '有', '传', '统', '收', '藏', '门', '类', '中', '的', '第', '一', '大', '户', '，', '只', '是', '我', '们', '结', '束', '温', '饱', '的', '时', '间', '太', '短', '而', '已', '。'], 'labels': [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]}\n",
      "{'tokens': ['因', '有', '关', '日', '寇', '在', '京', '掠', '夺', '文', '物', '详', '情', '，', '藏', '界', '较', '为', '重', '视', '，', '也', '是', '我', '们', '收', '藏', '北', '京', '史', '料', '中', '的', '要', '件', '之', '一', '。'], 'labels': [6, 6, 6, 4, 6, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6]}\n",
      "{'tokens': ['我', '们', '藏', '有', '一', '册', '１', '９', '４', '５', '年', '６', '月', '油', '印', '的', '《', '北', '京', '文', '物', '保', '存', '保', '管', '状', '态', '之', '调', '查', '报', '告', '》', '，', '调', '查', '范', '围', '涉', '及', '故', '宫', '、', '历', '博', '、', '古', '研', '所', '、', '北', '大', '清', '华', '图', '书', '馆', '、', '北', '图', '、', '日', '伪', '资', '料', '库', '等', '二', '十', '几', '家', '，', '言', '及', '文', '物', '二', '十', '万', '件', '以', '上', '，', '洋', '洋', '三', '万', '余', '言', '，', '是', '珍', '贵', '的', '北', '京', '史', '料', '。'], 'labels': [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 6, 4, 5, 6, 2, 3, 3, 6, 4, 5, 5, 5, 5, 5, 5, 6, 4, 5, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 6, 6, 6]}\n",
      "{'tokens': ['以', '家', '乡', '的', '历', '史', '文', '献', '、', '特', '定', '历', '史', '时', '期', '书', '刊', '、', '某', '一', '名', '家', '或', '名', '著', '的', '多', '种', '出', '版', '物', '为', '专', '题', '，', '注', '意', '精', '品', '、', '非', '卖', '品', '、', '纪', '念', '品', '，', '集', '成', '系', '列', '，', '那', '收', '藏', '的', '过', '程', '就', '已', '经', '够', '您', '玩', '味', '无', '穷', '了', '。'], 'labels': [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "每条数据包含一句文本和这个文本中每个汉字以及数字对应的label标签。\n",
    "\n",
    "之后，还需要对输入句子进行数据处理，如切词，映射词表id等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据处理\n",
    "\n",
    "预训练模型ERNIE对中文数据的处理是以字为单位。PaddleNLP对于各种预训练模型已经内置了相应的tokenizer。指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "tokenizer作用为将原始输入文本转化成模型model可以接受的输入数据形式。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "<br><center>图3：ERNIE模型示意图</center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp.transformers\r\n",
    "import paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def convert_example(example, tokenizer):\r\n",
    "#     tokens, labels = example['tokens'],example['labels']\r\n",
    "#     tokenized_input = tokenizer(\r\n",
    "#         tokens, return_length=True, is_split_into_words=True)\r\n",
    "#     # Token '[CLS]' and '[SEP]' will get label 'O'\r\n",
    "#     labels = ['O'] + labels + ['O']\r\n",
    "#     tokenized_input['labels'] = labels\r\n",
    "#     return tokenized_input['input_ids'], tokenized_input[\r\n",
    "#         'token_type_ids'], tokenized_input['seq_len'], tokenized_input['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-11 21:12:52,111] [    INFO] - Found /home/aistudio/.paddlenlp/models/roberta-wwm-ext/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import random\r\n",
    "len_limit=256\r\n",
    "def filter_fn(data):\r\n",
    "    if len(data['tokens']) > len_limit:\r\n",
    "        return False\r\n",
    "    if sum(data['labels']) == 6 * len(data['tokens']):\r\n",
    "        if random.random()<0.7:\r\n",
    "            return False\r\n",
    "        else:\r\n",
    "            return True\r\n",
    "    return True\r\n",
    "\r\n",
    "train_ds = train_ds.filter(filter_fn)\r\n",
    "test_ds = test_ds.filter(filter_fn)\r\n",
    "\r\n",
    "tokenizer = paddlenlp.transformers.RobertaTokenizer.from_pretrained('roberta-wwm-ext')\r\n",
    "# tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-11 23:01:42,839] [    INFO] - Found /home/aistudio/.paddlenlp/models/roberta-wwm-ext/vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 52, [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "# label_vocab = load_dict('./data/tag.dic')\n",
    "# # # tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')\n",
    "tokenizer = paddlenlp.transformers.RobertaTokenizer.from_pretrained('roberta-wwm-ext')\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\n",
    "\n",
    "train_ds.map(trans_func)\n",
    "dev_ds.map(trans_func)\n",
    "test_ds.map(trans_func)\n",
    "print (train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据读入\n",
    "\n",
    "使用`paddle.io.DataLoader`接口多线程异步加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_label = -1\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(),  # seq_len\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\n",
    "): fn(samples)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "dev_loader = paddle.io.DataLoader(\n",
    "    dataset=dev_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "test_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PaddleNLP一键加载预训练模型\n",
    "\n",
    "\n",
    "快递单信息抽取本质是一个序列标注任务，PaddleNLP对于各种预训练模型已经内置了对于下游任务文本分类Fine-tune网络。以下教程以ERNIE为预训练模型完成序列标注任务。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification()`一行代码即可加载预训练模型ERNIE用于序列标注任务的fine-tune网络。其在ERNIE模型后拼接上一个全连接网络进行分类。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification.from_pretrained()`方法只需指定想要使用的模型名称和文本分类的类别数即可完成定义模型网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这里换用了Roberta模型\n",
    "\n",
    "使用的时候把token和pretrained模型换掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-11 23:01:50,883] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext/roberta_chn_base.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "# Define the model netword and its loss\n",
    "# model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_vocab))\n",
    "model = paddlenlp.transformers.RobertaForTokenClassification.from_pretrained(\"roberta-wwm-ext\", num_classes=len(label_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PaddleNLP不仅支持ERNIE预训练模型，还支持BERT、RoBERTa、Electra等预训练模型。\n",
    "下表汇总了目前PaddleNLP支持的各类预训练模型。您可以使用PaddleNLP提供的模型，完成文本分类、序列标注、问答等任务。同时我们提供了众多预训练模型的参数权重供用户使用，其中包含了二十多种中文语言模型的预训练权重。中文的预训练模型有`bert-base-chinese, bert-wwm-chinese, bert-wwm-ext-chinese, ernie-1.0, ernie-tiny, gpt2-base-cn, roberta-wwm-ext, roberta-wwm-ext-large, rbt3, rbtl3, chinese-electra-base, chinese-electra-small, chinese-xlnet-base, chinese-xlnet-mid, chinese-xlnet-large, unified_transformer-12L-cn, unified_transformer-12L-cn-luge`等。\n",
    "\n",
    "更多预训练模型参考：[PaddleNLP Transformer API](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/transformers.md)。\n",
    "\n",
    "更多预训练模型fine-tune下游任务使用方法，请参考：[examples](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 设置Fine-Tune优化策略，模型配置\n",
    "适用于ERNIE/BERT这类Transformer模型的迁移优化学习率策略为warmup的动态学习率。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/2bc624280a614a80b5449773192be460f195b13af89e4e5cbaf62bf6ac16de2c\" width=\"40%\" height=\"30%\"/> <br />\n",
    "</p><br><center>图4：动态学习率示意图</center></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "loss_fn = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练与评估\n",
    "\n",
    "\n",
    "模型训练的过程通常有以下步骤：\n",
    "\n",
    "1. 从dataloader中取出一个batch data\n",
    "2. 将batch data喂给model，做前向计算\n",
    "3. 将前向计算结果传给损失函数，计算loss。将前向计算结果传给评价方法，计算评价指标。\n",
    "4. loss反向回传，更新梯度。重复以上步骤。\n",
    "\n",
    "每训练一个epoch时，程序将会评估一次，评估当前模型训练的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这里添加了Visual DL的代码，可以得到Loss变换的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval precision: 0.972287 - recall: 0.954745 - f1: 0.963436\r"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "from visualdl import LogWriter\n",
    "import numpy as np\n",
    "writer = LogWriter(\"./log\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    for idx, (input_ids, token_type_ids, length, labels) in enumerate(train_loader):\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = paddle.mean(loss_fn(logits, labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        step += 1\n",
    "        print(\"epoch:%d - step:%d - loss: %f\" % (epoch, step, loss))\n",
    "        writer.add_scalar(tag=\"eval/loss\", step=step, value=loss)\n",
    "\n",
    "    evaluate(model, metric, dev_loader)\n",
    "\n",
    "\n",
    "    paddle.save(model.state_dict(),\n",
    "                './ernie_result/model_%d.pdparams' % step)\n",
    "    # paddle.save(model.state_dict(),\n",
    "    #             './reberta_result/model_%d.pdparams' % step)\n",
    "# model.save_pretrained('./checkpoint')\n",
    "# tokenizer.save_pretrained('./checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Visual DL结果：\n",
    "\n",
    "数据集是默认的waybill快递单数据集，使用的是roberta模型\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1a308614119b4823b24eff9c5e818dfecabbb5995b134e94b61430e3c4f64a0a)\n",
    "\n",
    "换用了微软的数据集以后得到的结果，使用的是roberta模型，可以看出来数据多了很多很多\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/031966e009314fedbd361c7983fad5dd38eb86739b834008996eefc0b024f1f8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir checkpoint3/\r\n",
    "model.save_pretrained('./checkpoint3')\r\n",
    "tokenizer.save_pretrained('./checkpoint3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型预测\n",
    "\n",
    "训练保存好的模型，即可用于预测。如以下示例代码自定义预测数据，调用`predict()`函数即可一键预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results have been saved in the file: roberta_results.txt, some examples are shown below: \n",
      "('黑龙江省', 'A1')('双鸭山市', 'A2')('尖山区', 'A3')('八马路与东平行路交叉口北40米', 'A4')('韦业涛', 'P')('18600009172', 'T')\n",
      "('广西壮族自治区', 'A1')('桂林市', 'A2')('雁山区', 'A3')('雁山镇西龙村老年活动中心', 'A4')('17610348888', 'T')('羊卓卫', 'P')\n",
      "('15652864561', 'T')('河南省', 'A1')('开封市', 'A2')('顺河回族区', 'A3')('顺河区公园路32号', 'A4')('赵本山', 'P')\n",
      "('河北省', 'A1')('唐山市', 'A2')('玉田县', 'A3')('无终大街159号', 'A4')('18614253058', 'T')('尚汉生', 'P')\n",
      "('台湾', 'A1')('台中市', 'A2')('北区', 'A3')('北区锦新街18号', 'A4')('18511226708', 'T')('蓟丽', 'P')\n",
      "('廖梓琪', 'P')('18514743222', 'T')('湖北省', 'A1')('宜昌市', 'A2')('长阳土家族自治县', 'A3')('贺家坪镇贺家坪村一组临河1号', 'A4')\n",
      "('江苏省', 'A1')('南通市', 'A2')('海门市', 'A3')('孝威村孝威路88号', 'A4')('18611840623', 'T')('计星仪', 'P')\n",
      "('17601674746', 'T')('赵春丽', 'P')('内蒙古自治区', 'A1')('乌兰察布市', 'A2')('凉城县', 'A3')('新建街', 'A4')\n",
      "('云南省', 'A1')('临沧市', 'A2')('耿马傣族佤族自治县', 'A3')('鑫源路法院对面', 'A4')('许贞爱', 'P')('18510566685', 'T')\n",
      "('四川省', 'A1')('成都市', 'A2')('双流区', 'A3')('东升镇北仓路196号', 'A4')('耿丕岭', 'P')('18513466161', 'T')\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, test_loader, test_ds, label_vocab)\n",
    "# file_path = \"ernie_results.txt\"\n",
    "file_path = \"roberta_results.txt\"\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write(\"\\n\".join(preds))\n",
    "# Print some examples\n",
    "print(\n",
    "    \"The results have been saved in the file: %s, some examples are shown below: \"\n",
    "    % file_path)\n",
    "print(\"\\n\".join(preds[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这里没有修改好的话会得到全是other的结果，不过这次显然计算结果是正常的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results have been saved in the file: roberta_results.txt, some examples are shown below: \n",
      "('中共中央', 'ORG')('致', 'O')('中国致公党', 'ORG')('十一大', 'O')('的', 'O')('贺', 'O')('词', 'O')('各', 'O')('位', 'O')('代', 'O')('表', 'O')('、', 'O')('各', 'O')('位', 'O')('同', 'O')('志', 'O')('：', 'O')('在', 'O')('中国致公党第十一次全国代表大会', 'ORG')('隆', 'O')('重', 'O')('召', 'O')('开', 'O')('之', 'O')('际', 'O')('，', 'O')('中国共产党中央委员会', 'ORG')('谨', 'O')('向', 'O')('大', 'O')('会', 'O')('表', 'O')('示', 'O')('热', 'O')('烈', 'O')('的', 'O')('祝', 'O')('贺', 'O')('，', 'O')('向', 'O')('致公党', 'ORG')('的', 'O')('同', 'O')('志', 'O')('们', 'O')('致', 'O')('以', 'O')('亲', 'O')('切', 'O')('的', 'O')('问', 'O')('候', 'O')('！', 'O')\n",
      "('在', 'O')('过', 'O')('去', 'O')('的', 'O')('五', 'O')('年', 'O')('中', 'O')('，', 'O')('致公党', 'ORG')('在', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('指', 'O')('引', 'O')('下', 'O')('，', 'O')('遵', 'O')('循', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('初', 'O')('级', 'O')('阶', 'O')('段', 'O')('的', 'O')('基', 'O')('本', 'O')('路', 'O')('线', 'O')('，', 'O')('努', 'O')('力', 'O')('实', 'O')('践', 'O')('致公', 'ORG')('党', 'O')('十', 'O')('大', 'O')('提', 'O')('出', 'O')('的', 'O')('发', 'O')('挥', 'O')('参', 'O')('政', 'O')('党', 'O')('职', 'O')('能', 'O')('、', 'O')('加', 'O')('强', 'O')('自', 'O')('身', 'O')('建', 'O')('设', 'O')('的', 'O')('基', 'O')('本', 'O')('任', 'O')('务', 'O')('。', 'O')\n",
      "('高', 'O')('举', 'O')('爱', 'O')('国', 'O')('主', 'O')('义', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('两', 'O')('面', 'O')('旗', 'O')('帜', 'O')('，', 'O')('团', 'O')('结', 'O')('全', 'O')('体', 'O')('成', 'O')('员', 'O')('以', 'O')('及', 'O')('所', 'O')('联', 'O')('系', 'O')('的', 'O')('归', 'O')('侨', 'O')('、', 'O')('侨', 'O')('眷', 'O')('，', 'O')('发', 'O')('扬', 'O')('爱', 'O')('国', 'O')('革', 'O')('命', 'O')('的', 'O')('光', 'O')('荣', 'O')('传', 'O')('统', 'O')('，', 'O')('为', 'O')('统', 'O')('一', 'O')('祖', 'O')('国', 'O')('、', 'O')('振', 'O')('兴', 'O')('中华', 'LOC')('而', 'O')('努', 'O')('力', 'O')('奋', 'O')('斗', 'O')('；', 'O')('紧', 'O')('紧', 'O')('围', 'O')('绕', 'O')('国', 'O')('家', 'O')('的', 'O')('中', 'O')('心', 'O')('工', 'O')('作', 'O')('，', 'O')('联', 'O')('系', 'O')('改', 'O')('革', 'O')('和', 'O')('建', 'O')('设', 'O')('中', 'O')('的', 'O')('重', 'O')('大', 'O')('问', 'O')('题', 'O')('以', 'O')('及', 'O')('人', 'O')('民', 'O')('群', 'O')('众', 'O')('普', 'O')('遍', 'O')('关', 'O')('心', 'O')('的', 'O')('社', 'O')('会', 'O')('问', 'O')('题', 'O')('，', 'O')('深', 'O')('入', 'O')('开', 'O')('展', 'O')('调', 'O')('查', 'O')('研', 'O')('究', 'O')('，', 'O')('就', 'O')('经', 'O')('济', 'O')('建', 'O')('设', 'O')('、', 'O')('侨', 'O')('务', 'O')('政', 'O')('策', 'O')('、', 'O')('文', 'O')('教', 'O')('卫', 'O')('生', 'O')\n",
      "('在', 'O')('此', 'O')('，', 'O')('中共中央', 'ORG')('谨', 'O')('向', 'O')('致公党中央', 'ORG')('以', 'O')('及', 'O')('全', 'O')('体', 'O')('成', 'O')('员', 'O')('致', 'O')('以', 'O')('崇', 'O')('高', 'O')('的', 'O')('敬', 'O')('意', 'O')('！', 'O')\n",
      "('不', 'O')('久', 'O')('前', 'O')('，', 'O')('中国共产党', 'ORG')('召', 'O')('开', 'O')('了', 'O')('举', 'O')('世', 'O')('瞩', 'O')('目', 'O')('的', 'O')('第十五次全国代表大会', 'ORG')('。', 'O')\n",
      "('这', 'O')('次', 'O')('代', 'O')('表', 'O')('大', 'O')('会', 'O')('是', 'O')('在', 'O')('中国', 'LOC')('改', 'O')('革', 'O')('开', 'O')('放', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('现', 'O')('代', 'O')('化', 'O')('建', 'O')('设', 'O')('发', 'O')('展', 'O')('的', 'O')('关', 'O')('键', 'O')('时', 'O')('刻', 'O')('召', 'O')('开', 'O')('的', 'O')('历', 'O')('史', 'O')('性', 'O')('会', 'O')('议', 'O')('。', 'O')\n",
      "('大', 'O')('会', 'O')('高', 'O')('举', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('伟', 'O')('大', 'O')('旗', 'O')('帜', 'O')('，', 'O')('回', 'O')('顾', 'O')('一', 'O')('个', 'O')('世', 'O')('纪', 'O')('以', 'O')('来', 'O')('中国', 'LOC')('人', 'O')('民', 'O')('的', 'O')('奋', 'O')('斗', 'O')('历', 'O')('史', 'O')('，', 'O')('展', 'O')('望', 'O')('下', 'O')('个', 'O')('世', 'O')('纪', 'O')('５', 'O')('０', 'O')('年', 'O')('的', 'O')('发', 'O')('展', 'O')('前', 'O')('景', 'O')('，', 'O')('认', 'O')('真', 'O')('总', 'O')('结', 'O')('了', 'O')('中共', 'ORG')('十', 'O')('一', 'O')('届', 'O')('三', 'O')('中', 'O')('全', 'O')('会', 'O')('以', 'O')('来', 'O')('特', 'O')('别', 'O')('是', 'O')('十四大', 'ORG')('以', 'O')('来', 'O')('的', 'O')('实', 'O')('践', 'O')('经', 'O')('验', 'O')('，', 'O')('对', 'O')('中国', 'LOC')('改', 'O')('革', 'O')('开', 'O')('放', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('现', 'O')('代', 'O')('化', 'O')('建', 'O')('设', 'O')('跨', 'O')('世', 'O')('纪', 'O')('的', 'O')('发', 'O')('展', 'O')('作', 'O')('出', 'O')('了', 'O')('全', 'O')('面', 'O')('部', 'O')('署', 'O')('。', 'O')\n",
      "('这', 'O')('次', 'O')('大', 'O')('会', 'O')('对', 'O')('于', 'O')('动', 'O')('员', 'O')('全', 'O')('党', 'O')('和', 'O')('全', 'O')('国', 'O')('各', 'O')('族', 'O')('人', 'O')('民', 'O')('，', 'O')('解', 'O')('放', 'O')('思', 'O')('想', 'O')('，', 'O')('实', 'O')('事', 'O')('求', 'O')('是', 'O')('，', 'O')('抓', 'O')('住', 'O')('有', 'O')('利', 'O')('时', 'O')('机', 'O')('，', 'O')('继', 'O')('续', 'O')('开', 'O')('拓', 'O')('前', 'O')('进', 'O')('，', 'O')('把', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('伟', 'O')('大', 'O')('事', 'O')('业', 'O')('全', 'O')('面', 'O')('推', 'O')('向', 'O')('２', 'O')('１', 'O')('世', 'O')('纪', 'O')('，', 'O')('具', 'O')('有', 'O')('极', 'O')('其', 'O')('重', 'O')('大', 'O')('和', 'O')('深', 'O')('远', 'O')('的', 'O')('意', 'O')('义', 'O')('。', 'O')\n",
      "('当', 'O')('前', 'O')('，', 'O')('在', 'O')('中共十五大', 'ORG')('精', 'O')('神', 'O')('的', 'O')('指', 'O')('引', 'O')('下', 'O')('，', 'O')('在', 'O')('以', 'O')('江泽民', 'PER')('同', 'O')('志', 'O')('为', 'O')('核', 'O')('心', 'O')('的', 'O')('中共中央', 'ORG')('领', 'O')('导', 'O')('下', 'O')('，', 'O')('全', 'O')('党', 'O')('和', 'O')('全', 'O')('国', 'O')('各', 'O')('族', 'O')('人', 'O')('民', 'O')('正', 'O')('高', 'O')('举', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('伟', 'O')('大', 'O')('旗', 'O')('帜', 'O')('，', 'O')('同', 'O')('心', 'O')('同', 'O')('德', 'O')('，', 'O')('团', 'O')('结', 'O')('奋', 'O')('斗', 'O')('，', 'O')('沿', 'O')('着', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('的', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('道', 'O')('路', 'O')('阔', 'O')('步', 'O')('前', 'O')('进', 'O')('。', 'O')\n",
      "('实', 'O')('现', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('的', 'O')('宏', 'O')('伟', 'O')('目', 'O')('标', 'O')('，', 'O')('是', 'O')('中国共产党', 'ORG')('和', 'O')('作', 'O')('为', 'O')('参', 'O')('政', 'O')('党', 'O')('的', 'O')('各', 'O')('民', 'O')('主', 'O')('党', 'O')('派', 'O')('共', 'O')('同', 'O')('肩', 'O')('负', 'O')('的', 'O')('历', 'O')('史', 'O')('使', 'O')('命', 'O')('。', 'O')\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, test_loader, test_ds, label_vocab)\r\n",
    "# file_path = \"ernie_results.txt\"\r\n",
    "file_path = \"roberta_results.txt\"\r\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\r\n",
    "    fout.write(\"\\n\".join(preds))\r\n",
    "# Print some examples\r\n",
    "print(\r\n",
    "    \"The results have been saved in the file: %s, some examples are shown below: \"\r\n",
    "    % file_path)\r\n",
    "print(\"\\n\".join(preds[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 进一步使用CRF\n",
    "\n",
    "PaddleNLP提供了CRF Layer，它能够学习label之间的关系，能够帮助模型更好地学习、预测序列标注任务。\n",
    "\n",
    "我们在PaddleNLP仓库中提供了[示例](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/examples/information_extraction/waybill_ie/run_ernie_crf.py)，您可以参照示例代码使用Ernie-CRF结构完成快递单信息抽取任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入交流群，一起学习吧\n",
    "\n",
    "现在就加入课程QQ交流群，一起交流NLP技术吧！\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394\" width=\"200\" height=\"250\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
