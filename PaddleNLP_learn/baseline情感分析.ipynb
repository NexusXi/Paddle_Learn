{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install --upgrade paddlenlp\n",
    "unzip -oq /home/aistudio/data/data95365/千言数据集情感分析.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先处理情感分析的数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1909/1909 [00:00<00:00, 43560.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错',\n",
       " 'label': 1,\n",
       " 'qid': ''}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "train_ds, dev_ds, test_ds = load_dataset(\"chnsenticorp\", splits=[\"train\", \"dev\", \"test\"])\n",
    "train_ds[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "load_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "def read(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        # 跳过列名\n",
    "        # next(f)\n",
    "        for line in f:\n",
    "            labels, words = line.strip('\\n').split('\\t')\n",
    "            labels = labels.split('\\002')\n",
    "            words = words.split('\\002')\n",
    "            yield {'text': words, 'label': labels}\n",
    "\n",
    "# data_path为read()方法的参数\n",
    "train_ds = load_dataset(read, data_path='dataset/ChnSentiCorp/train.tsv',lazy=False)\n",
    "# iter_ds = load_dataset(read, data_path='dataset/ChnSentiCorp/train.tsv',lazy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-17 10:47:50,348] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.pdparams and saved to /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch\n",
      "[2021-06-17 10:47:50,351] [    INFO] - Downloading skep_ernie_1.0_large_ch.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.pdparams\n",
      "100%|██████████| 1238309/1238309 [00:31<00:00, 38941.12it/s]\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-17 10:49:01,316] [    INFO] - Downloading skep_ernie_1.0_large_ch.vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.vocab.txt\n",
      "100%|██████████| 55/55 [00:00<00:00, 2779.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\n",
    "\n",
    "# 指定模型名称，一键加载模型\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=len(train_ds.label_list))\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "\n",
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False,\n",
    "                    dataset_name=\"chnsenticorp\"):\n",
    "    \"\"\"\n",
    "    Builds model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "    by concatenating and adding special tokens. And creates a mask from the two sequences passed \n",
    "    to be used in a sequence-pair classification task.\n",
    "        \n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence has the following format:\n",
    "    ::\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "\n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence pair mask has the following format:\n",
    "    ::\n",
    "\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence |\n",
    "\n",
    "    If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n",
    "    \n",
    "    note: There is no need token type ids for skep_roberta_large_ch model.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        example(obj:`list[str]`): List of input data, containing text and label if it have label.\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\n",
    "        max_seq_len(obj:`int`): The maximum total input sequence length after tokenization. \n",
    "            Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "        is_test(obj:`False`, defaults to `False`): Whether the example contains label or not.\n",
    "        dataset_name((obj:`str`, defaults to \"chnsenticorp\"): The dataset name, \"chnsenticorp\" or \"sst-2\".\n",
    "\n",
    "    Returns:\n",
    "        input_ids(obj:`list[int]`): The list of token ids.\n",
    "        token_type_ids(obj: `list[int]`): List of sequence pair mask.\n",
    "        label(obj:`numpy.array`, data type of int64, optional): The input label if not is_test.\n",
    "    \"\"\"\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"],\n",
    "        text_pair=example[\"text_pair\"],\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a759a8e722fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mStack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int64\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m ): [data for data in fn(samples)]\n\u001b[0;32m---> 19\u001b[0;31m train_data_loader = create_dataloader(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# 处理的最大文本序列长度\n",
    "max_seq_length=256\n",
    "# 批量数据大小\n",
    "batch_size=16\n",
    "\n",
    "# 将数据处理成model可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(dtype=\"int64\")  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-be47dcc7479a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/ChnSentiCorp/train.tsv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-be47dcc7479a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, mode)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mis_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"test\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-be47dcc7479a>\u001b[0m in \u001b[0;36m_read_file\u001b[0;34m(self, data_path, is_test)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                     \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0;31m# examples.append(('text',text,'label',label))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '1'"
     ]
    }
   ],
   "source": [
    "from paddle.io import Dataset\n",
    "from paddlenlp.datasets import MapDataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, mode=\"train\"):\n",
    "        is_test = True if mode == \"test\" else False\n",
    "        self.label_map = { item:index for index, item in enumerate(self.label_list)}\n",
    "        self.examples = self._read_file(data_path, is_test)\n",
    "\n",
    "    def _read_file(self, data_path, is_test):\n",
    "        examples = []\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if is_test:\n",
    "                    text = line.strip()\n",
    "                    examples.append((text,))\n",
    "                    # examples.append(('text',text,))\n",
    "                    # examples.append({'text': text})\n",
    "\n",
    "                else:\n",
    "                    label, text = line.strip('\\t').split('\\t')\n",
    "                    label = self.label_map[label]\n",
    "                    examples.append((label, text))\n",
    "                    # examples.append(('text',text,'label',label))\n",
    "                    # examples.append({'text': text, 'label': label})\n",
    "        return examples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    @property\n",
    "    def label_list(self):\n",
    "        return ['积极','消极']\n",
    "\n",
    "\n",
    "train_ds = MyDataset('dataset/ChnSentiCorp/train.tsv',mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import paddlenlp\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_func(file_path):\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\n",
    "\n",
    "data_dict = {'chnsenticorp': {'test': open_func('dataset/ChnSentiCorp/test.tsv'),\n",
    "                              'dev': open_func('dataset/ChnSentiCorp/dev.tsv'),\n",
    "                              'train': open_func('dataset/ChnSentiCorp/train.tsv')},\n",
    "             'nlpcc14sc': {'test': open_func('dataset/NLPCC14-SC/test.tsv'),\n",
    "                           'train': open_func('dataset/NLPCC14-SC/train.tsv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "from paddle.io import Dataset, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "import numpy as np\n",
    "label_list = [0, 1]\n",
    "\n",
    "# 注意，由于token type在此项任务中并没有起作用，因此这里不再考虑，让模型自行填充。\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "        self._for_test = for_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        samples = self._data[idx].split('\\t')\n",
    "        label = samples[-2]\n",
    "        text = samples[-1]\n",
    "        label = int(label)\n",
    "        text = self._tokenizer.encode(text, max_seq_len=self._max_len)['input_ids']\n",
    "        if self._for_test:\n",
    "            return np.array(text, dtype='int64')\n",
    "        else:\n",
    "            return np.array(text, dtype='int64'), np.array(label, dtype='int64')\n",
    "\n",
    "def batchify_fn(for_test=False):\n",
    "    if for_test:\n",
    "        return lambda samples, fn=Pad(axis=0, pad_val=tokenizer.pad_token_id): np.row_stack([data for data in fn(samples)])\n",
    "    else:\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "                                        Stack()): [data for data in fn(samples)]\n",
    "\n",
    "\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\n",
    "    shuffle = True if not for_test else False\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-20 10:47:04,656] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-20 10:47:15,119] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from paddle.static import InputSpec\n",
    "\n",
    "# 模型和分词\n",
    "model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\n",
    "\n",
    "# 参数设置\n",
    "data_name = 'nlpcc14sc'  # 更改此选项改变数据集\n",
    "\n",
    "## 训练相关\n",
    "epochs = 4\n",
    "learning_rate = 2e-5\n",
    "batch_size = 8\n",
    "max_len = 512\n",
    "\n",
    "## 数据相关\n",
    "train_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\n",
    "if data_name == 'chnsenticorp':\n",
    "    dev_dataloader = get_data_loader(data_dict[data_name]['dev'], tokenizer, batch_size, max_len, for_test=False)\n",
    "else:\n",
    "    dev_dataloader = None\n",
    "\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\n",
    "label = InputSpec((-1, 2), dtype='int64', name='label')\n",
    "model = paddle.Model(model, [input], [label])\n",
    "\n",
    "# 模型准备\n",
    "\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[paddle.metric.Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  200/1250 - loss: 0.4719 - acc: 0.7581 - 317ms/step\n",
      "step  400/1250 - loss: 0.7477 - acc: 0.7866 - 314ms/step\n",
      "step  600/1250 - loss: 0.0970 - acc: 0.7917 - 306ms/step\n",
      "step  800/1250 - loss: 0.3729 - acc: 0.7975 - 307ms/step\n",
      "step 1000/1250 - loss: 0.4014 - acc: 0.8015 - 311ms/step\n",
      "step 1200/1250 - loss: 0.4699 - acc: 0.8056 - 307ms/step\n",
      "step 1250/1250 - loss: 0.2398 - acc: 0.8061 - 310ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints_nlpcc14sc/0\n",
      "Epoch 2/4\n",
      "step  200/1250 - loss: 0.3707 - acc: 0.8719 - 323ms/step\n",
      "step  400/1250 - loss: 0.2640 - acc: 0.8747 - 311ms/step\n",
      "step  600/1250 - loss: 0.5327 - acc: 0.8760 - 306ms/step\n",
      "step  800/1250 - loss: 0.4280 - acc: 0.8744 - 312ms/step\n",
      "step 1000/1250 - loss: 0.3834 - acc: 0.8749 - 308ms/step\n",
      "step 1200/1250 - loss: 0.1221 - acc: 0.8753 - 306ms/step\n",
      "step 1250/1250 - loss: 0.1546 - acc: 0.8749 - 307ms/step\n",
      "Epoch 3/4\n",
      "step  200/1250 - loss: 0.4590 - acc: 0.9350 - 310ms/step\n",
      "step  400/1250 - loss: 0.0202 - acc: 0.9331 - 316ms/step\n",
      "step  600/1250 - loss: 0.4712 - acc: 0.9300 - 307ms/step\n",
      "step  800/1250 - loss: 0.0976 - acc: 0.9292 - 311ms/step\n",
      "step 1000/1250 - loss: 0.2000 - acc: 0.9283 - 310ms/step\n",
      "step 1200/1250 - loss: 0.0059 - acc: 0.9263 - 310ms/step\n",
      "step 1250/1250 - loss: 0.0483 - acc: 0.9261 - 309ms/step\n",
      "Epoch 4/4\n",
      "step  200/1250 - loss: 0.0337 - acc: 0.9694 - 327ms/step\n",
      "step  400/1250 - loss: 0.6804 - acc: 0.9712 - 306ms/step\n",
      "step  600/1250 - loss: 0.0793 - acc: 0.9644 - 311ms/step\n",
      "step  800/1250 - loss: 0.0139 - acc: 0.9652 - 312ms/step\n",
      "step 1000/1250 - loss: 0.2690 - acc: 0.9630 - 307ms/step\n",
      "step 1200/1250 - loss: 0.0283 - acc: 0.9625 - 307ms/step\n",
      "step 1250/1250 - loss: 0.0098 - acc: 0.9621 - 307ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints_nlpcc14sc/final\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 开始训练\n",
    "model.fit(train_dataloader, dev_dataloader, batch_size, epochs, eval_freq=5, save_freq=800, save_dir='./checkpoints_nlpcc14sc', log_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-20 11:14:17,819] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 导入预训练模型\n",
    "checkpoint_path = './checkpoints_nlpcc14sc/final'  # 填写预训练模型的保存路径\n",
    "\n",
    "model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\n",
    "model = paddle.Model(model, input)\n",
    "model.load(checkpoint_path)\n",
    "\n",
    "# 导入测试集\n",
    "test_dataloader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_len, for_test=True)\n",
    "# 预测保存\n",
    "\n",
    "save_file = {'chnsenticorp': './submission/ChnSentiCorp.tsv', 'nlpcc14sc': './submission/NLPCC14-SC.tsv'}\n",
    "predicts = []\n",
    "for batch in test_dataloader:\n",
    "    predict = model.predict_batch(batch)\n",
    "    predicts += predict[0].argmax(axis=-1).tolist()\n",
    "\n",
    "with open(save_file[data_name], 'w', encoding='utf8') as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for idx, sample in enumerate(data_dict[data_name]['test']):\n",
    "        qid = sample.split('\\t')[0]\n",
    "        f.write(qid + '\\t' + str(predicts[idx]) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddlenlp\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_func(file_path):\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\n",
    "\n",
    "data_dict = {'seabsa16phns': {'test': open_func('dataset/SE-ABSA16_PHNS/test.tsv'),\n",
    "                              'train': open_func('dataset/SE-ABSA16_PHNS/train.tsv')},\n",
    "             'seabsa16came': {'test': open_func('dataset/SE-ABSA16_CAME/test.tsv'),\n",
    "                              'train': open_func('dataset/SE-ABSA16_CAME/train.tsv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "from paddle.io import Dataset, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "import numpy as np\n",
    "label_list = [0, 1]\n",
    "\n",
    "# 考虑token_type_id\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "        self._for_test = for_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        samples = self._data[idx].split('\\t')\n",
    "        label = samples[-3]\n",
    "        text_b = samples[-1]\n",
    "        text_a = samples[-2]\n",
    "        label = int(label)\n",
    "        encoder_out = self._tokenizer.encode(text_a, text_b, max_seq_len=self._max_len)\n",
    "        text = encoder_out['input_ids']\n",
    "        token_type = encoder_out['token_type_ids']\n",
    "        if self._for_test:\n",
    "            return np.array(text, dtype='int64'), np.array(token_type, dtype='int64')\n",
    "        else:\n",
    "            return np.array(text, dtype='int64'), np.array(token_type, dtype='int64'), np.array(label, dtype='int64')\n",
    "\n",
    "def batchify_fn(for_test=False):\n",
    "    if for_test:\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "                                        Pad(axis=0, pad_val=tokenizer.pad_token_type_id)): [data for data in fn(samples)]\n",
    "    else:\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "                                        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "                                        Stack()): [data for data in fn(samples)]\n",
    "\n",
    "\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\n",
    "    shuffle = True if not for_test else False\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-20 11:42:15,361] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "[2021-06-20 11:42:20,213] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from paddle.static import InputSpec\n",
    "\n",
    "# 模型和分词\n",
    "model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\n",
    "\n",
    "# 参数设置\n",
    "data_name = 'seabsa16came'  # 更改此选项改变数据集\n",
    "\n",
    "## 训练相关\n",
    "epochs = 4\n",
    "learning_rate = 2e-5\n",
    "batch_size = 8\n",
    "max_len = 512\n",
    "\n",
    "## 数据相关\n",
    "train_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\n",
    "\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\n",
    "token_type = InputSpec((-1, -1), dtype='int64', name='token_type')\n",
    "label = InputSpec((-1, 2), dtype='int64', name='label')\n",
    "model = paddle.Model(model, [input, token_type], [label])\n",
    "\n",
    "# 模型准备\n",
    "\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[paddle.metric.Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/4\n",
      "step 165/165 - loss: 0.4399 - acc: 0.6287 - 815ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints_seabsa16came/0\n",
      "Epoch 2/4\n",
      "step 165/165 - loss: 0.2631 - acc: 0.6743 - 817ms/step\n",
      "Epoch 3/4\n",
      "step 165/165 - loss: 0.4135 - acc: 0.6986 - 818ms/step\n",
      "Epoch 4/4\n",
      "step 165/165 - loss: 0.6291 - acc: 0.7107 - 818ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints_seabsa16came/final\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataloader, batch_size=batch_size, epochs=epochs, save_freq=800, save_dir='./checkpoints_seabsa16came', log_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-20 11:52:45,062] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 导入预训练模型\n",
    "checkpoint_path = './checkpoints_seabsa16came/final'  # 填写预训练模型的保存路径\n",
    "\n",
    "model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\n",
    "token_type = InputSpec((-1, -1), dtype='int64', name='token_type')\n",
    "model = paddle.Model(model, [input, token_type])\n",
    "model.load(checkpoint_path)\n",
    "\n",
    "# 导入测试集\n",
    "test_dataloader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_len, for_test=True)\n",
    "# 预测保存\n",
    "\n",
    "save_file = {'seabsa16phns': './submission/SE-ABSA16_PHNS.tsv', 'seabsa16came': './submission/SE-ABSA16_CAME.tsv'}\n",
    "predicts = []\n",
    "for batch in test_dataloader:\n",
    "    predict = model.predict_batch(batch)\n",
    "    predicts += predict[0].argmax(axis=-1).tolist()\n",
    "\n",
    "with open(save_file[data_name], 'w', encoding='utf8') as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for idx, sample in enumerate(data_dict[data_name]['test']):\n",
    "        qid = sample.split('\\t')[0]\n",
    "        f.write(qid + '\\t' + str(predicts[idx]) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddlenlp\n",
    "from paddlenlp.transformers import SkepForTokenClassification, SkepTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_func(file_path):\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\n",
    "\n",
    "data_dict = {'cotebd': {'test': open_func('dataset/COTE-BD/test.tsv'),\n",
    "                        'train': open_func('dataset/COTE-BD/train.tsv')},\n",
    "             'cotedp': {'test': open_func('dataset/COTE-DP/test.tsv'),\n",
    "                        'train': open_func('dataset/COTE-DP/train.tsv')},\n",
    "             'cotemfw': {'test': open_func('dataset/COTE-MFW/test.tsv'),\n",
    "                        'train': open_func('dataset/COTE-MFW/train.tsv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "from paddle.io import Dataset, DataLoader\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "import numpy as np\n",
    "label_list = {'B': 0, 'I': 1, 'O': 2}\n",
    "index2label = {0: 'B', 1: 'I', 2: 'O'}\n",
    "\n",
    "# 考虑token_type_id\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "        self._for_test = for_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        samples = self._data[idx].split('\\t')\n",
    "        label = samples[-2]\n",
    "        text = samples[-1]\n",
    "        if self._for_test:\n",
    "            origin_enc = self._tokenizer.encode(text, max_seq_len=self._max_len)['input_ids']\n",
    "            return np.array(origin_enc, dtype='int64')\n",
    "        else:\n",
    "            \n",
    "            # 由于并不是每个字都是一个token，这里采用一种简单的处理方法，先编码label，再编码text中除了label以外的词，最后合到一起\n",
    "            texts = text.split(label)\n",
    "            label_enc = self._tokenizer.encode(label)['input_ids']\n",
    "            cls_enc = label_enc[0]\n",
    "            sep_enc = label_enc[-1]\n",
    "            label_enc = label_enc[1:-1]\n",
    "            \n",
    "            # 合并\n",
    "            origin_enc = []\n",
    "            label_ids = []\n",
    "            for index, text in enumerate(texts):\n",
    "                text_enc = self._tokenizer.encode(text)['input_ids']\n",
    "                text_enc = text_enc[1:-1]\n",
    "                origin_enc += text_enc\n",
    "                label_ids += [label_list['O']] * len(text_enc)\n",
    "                if index != len(texts) - 1:\n",
    "                    origin_enc += label_enc\n",
    "                    label_ids += [label_list['B']] + [label_list['I']] * (len(label_enc) - 1)\n",
    "\n",
    "            origin_enc = [cls_enc] + origin_enc + [sep_enc]\n",
    "            label_ids = [label_list['O']] + label_ids + [label_list['O']]\n",
    "            \n",
    "            # 截断\n",
    "            if len(origin_enc) > self._max_len:\n",
    "                origin_enc = origin_enc[:self._max_len-1] + origin_enc[-1:]\n",
    "                label_ids = label_ids[:self._max_len-1] + label_ids[-1:]\n",
    "            return np.array(origin_enc, dtype='int64'), np.array(label_ids, dtype='int64')\n",
    "\n",
    "\n",
    "def batchify_fn(for_test=False):\n",
    "    if for_test:\n",
    "        return lambda samples, fn=Pad(axis=0, pad_val=tokenizer.pad_token_id): np.row_stack([data for data in fn(samples)])\n",
    "    else:\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "                                        Pad(axis=0, pad_val=label_list['O'])): [data for data in fn(samples)]\n",
    "\n",
    "\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\n",
    "    shuffle = True if not for_test else False\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-20 13:24:59,948] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-20 13:25:10,315] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from paddle.static import InputSpec\n",
    "from paddlenlp.metrics import Perplexity\n",
    "\n",
    "# 模型和分词\n",
    "model = SkepForTokenClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=3)\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\n",
    "\n",
    "# 参数设置\n",
    "data_name = 'cotemfw'  # 更改此选项改变数据集\n",
    "\n",
    "## 训练相关\n",
    "epochs = 4\n",
    "learning_rate = 2e-5\n",
    "batch_size = 8\n",
    "max_len = 512\n",
    "\n",
    "## 数据相关\n",
    "train_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\n",
    "\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\n",
    "label = InputSpec((-1, -1, 3), dtype='int64', name='label')\n",
    "model = paddle.Model(model, [input], [label])\n",
    "\n",
    "# 模型准备\n",
    "\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[Perplexity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  200/5157 - loss: 0.0066 - Perplexity: 1.0869 - 196ms/step\n",
      "step  400/5157 - loss: 0.0090 - Perplexity: 1.0565 - 199ms/step\n",
      "step  800/5157 - loss: 0.0397 - Perplexity: 1.0407 - 198ms/step\n",
      "step 1000/5157 - loss: 0.0185 - Perplexity: 1.0375 - 198ms/step\n",
      "step 1200/5157 - loss: 0.0200 - Perplexity: 1.0355 - 197ms/step\n",
      "step 1400/5157 - loss: 0.0349 - Perplexity: 1.0334 - 197ms/step\n",
      "step 1600/5157 - loss: 0.0150 - Perplexity: 1.0320 - 198ms/step\n",
      "step 1800/5157 - loss: 0.0025 - Perplexity: 1.0309 - 197ms/step\n",
      "step 2000/5157 - loss: 0.0351 - Perplexity: 1.0300 - 197ms/step\n",
      "step 2200/5157 - loss: 0.0569 - Perplexity: 1.0294 - 197ms/step\n",
      "step 2400/5157 - loss: 0.0116 - Perplexity: 1.0287 - 196ms/step\n",
      "step 2600/5157 - loss: 0.0408 - Perplexity: 1.0282 - 196ms/step\n",
      "step 2800/5157 - loss: 0.0465 - Perplexity: 1.0276 - 195ms/step\n",
      "step 3000/5157 - loss: 0.0167 - Perplexity: 1.0271 - 196ms/step\n",
      "step 3200/5157 - loss: 0.0072 - Perplexity: 1.0266 - 195ms/step\n",
      "step 3400/5157 - loss: 0.0104 - Perplexity: 1.0264 - 195ms/step\n",
      "step 3600/5157 - loss: 0.0047 - Perplexity: 1.0260 - 195ms/step\n",
      "step 3800/5157 - loss: 0.0033 - Perplexity: 1.0255 - 195ms/step\n",
      "step 4000/5157 - loss: 0.0181 - Perplexity: 1.0255 - 195ms/step\n",
      "step 4200/5157 - loss: 0.0135 - Perplexity: 1.0251 - 195ms/step\n",
      "step 4400/5157 - loss: 0.0232 - Perplexity: 1.0248 - 195ms/step\n",
      "step 4600/5157 - loss: 0.0350 - Perplexity: 1.0247 - 195ms/step\n",
      "step 4800/5157 - loss: 0.0027 - Perplexity: 1.0245 - 195ms/step\n",
      "step 5000/5157 - loss: 0.0225 - Perplexity: 1.0242 - 195ms/step\n",
      "step 5157/5157 - loss: 6.5608e-04 - Perplexity: 1.0241 - 195ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints_cotemfw/0\n",
      "Epoch 2/4\n",
      "step  200/5157 - loss: 0.0075 - Perplexity: 1.0141 - 192ms/step\n",
      "step  400/5157 - loss: 0.0043 - Perplexity: 1.0132 - 193ms/step\n",
      "step  600/5157 - loss: 0.0089 - Perplexity: 1.0130 - 193ms/step\n",
      "step  800/5157 - loss: 0.0064 - Perplexity: 1.0128 - 193ms/step\n",
      "step 1000/5157 - loss: 0.0195 - Perplexity: 1.0128 - 193ms/step\n",
      "step 1200/5157 - loss: 0.0086 - Perplexity: 1.0130 - 193ms/step\n",
      "step 1400/5157 - loss: 0.0048 - Perplexity: 1.0126 - 194ms/step\n",
      "step 1600/5157 - loss: 0.0098 - Perplexity: 1.0129 - 193ms/step\n",
      "step 1800/5157 - loss: 0.0045 - Perplexity: 1.0129 - 193ms/step\n",
      "step 2000/5157 - loss: 0.0110 - Perplexity: 1.0130 - 193ms/step\n",
      "step 2200/5157 - loss: 0.0056 - Perplexity: 1.0129 - 194ms/step\n",
      "step 2400/5157 - loss: 0.0705 - Perplexity: 1.0130 - 195ms/step\n",
      "step 2600/5157 - loss: 0.0163 - Perplexity: 1.0129 - 194ms/step\n",
      "step 2800/5157 - loss: 0.0046 - Perplexity: 1.0129 - 195ms/step\n",
      "step 3000/5157 - loss: 0.0061 - Perplexity: 1.0128 - 194ms/step\n",
      "step 3200/5157 - loss: 0.0093 - Perplexity: 1.0129 - 194ms/step\n",
      "step 3400/5157 - loss: 0.0018 - Perplexity: 1.0129 - 194ms/step\n",
      "step 3600/5157 - loss: 0.0288 - Perplexity: 1.0128 - 194ms/step\n",
      "step 3800/5157 - loss: 9.4823e-04 - Perplexity: 1.0129 - 194ms/step\n",
      "step 4000/5157 - loss: 0.0044 - Perplexity: 1.0129 - 194ms/step\n",
      "step 4200/5157 - loss: 0.0050 - Perplexity: 1.0128 - 194ms/step\n",
      "step 4400/5157 - loss: 0.0039 - Perplexity: 1.0128 - 194ms/step\n",
      "step 4600/5157 - loss: 0.0021 - Perplexity: 1.0129 - 194ms/step\n",
      "step 4800/5157 - loss: 0.0130 - Perplexity: 1.0129 - 194ms/step\n",
      "step 5000/5157 - loss: 0.0035 - Perplexity: 1.0129 - 194ms/step\n",
      "step 5157/5157 - loss: 0.0498 - Perplexity: 1.0129 - 194ms/step\n",
      "Epoch 3/4\n",
      "step  200/5157 - loss: 0.0030 - Perplexity: 1.0080 - 192ms/step\n",
      "step  400/5157 - loss: 0.0120 - Perplexity: 1.0074 - 192ms/step\n",
      "step  600/5157 - loss: 0.0073 - Perplexity: 1.0075 - 192ms/step\n",
      "step  800/5157 - loss: 0.0299 - Perplexity: 1.0075 - 191ms/step\n",
      "step 1000/5157 - loss: 9.3076e-04 - Perplexity: 1.0073 - 192ms/step\n",
      "step 1200/5157 - loss: 0.0183 - Perplexity: 1.0075 - 192ms/step\n",
      "step 1400/5157 - loss: 4.0536e-04 - Perplexity: 1.0074 - 193ms/step\n",
      "step 1600/5157 - loss: 5.1199e-04 - Perplexity: 1.0073 - 194ms/step\n",
      "step 1800/5157 - loss: 0.0079 - Perplexity: 1.0076 - 193ms/step\n",
      "step 2000/5157 - loss: 0.0235 - Perplexity: 1.0076 - 193ms/step\n",
      "step 2200/5157 - loss: 0.0016 - Perplexity: 1.0076 - 193ms/step\n",
      "step 2400/5157 - loss: 0.0058 - Perplexity: 1.0076 - 193ms/step\n",
      "step 2600/5157 - loss: 0.0035 - Perplexity: 1.0077 - 193ms/step\n",
      "step 2800/5157 - loss: 0.0037 - Perplexity: 1.0076 - 194ms/step\n",
      "step 3000/5157 - loss: 0.0076 - Perplexity: 1.0077 - 194ms/step\n",
      "step 3200/5157 - loss: 0.0142 - Perplexity: 1.0078 - 194ms/step\n",
      "step 3400/5157 - loss: 0.0094 - Perplexity: 1.0078 - 194ms/step\n",
      "step 3600/5157 - loss: 0.0035 - Perplexity: 1.0078 - 195ms/step\n",
      "step 3800/5157 - loss: 0.0252 - Perplexity: 1.0078 - 195ms/step\n",
      "step 4000/5157 - loss: 0.0105 - Perplexity: 1.0079 - 195ms/step\n",
      "step 4200/5157 - loss: 0.0093 - Perplexity: 1.0079 - 195ms/step\n",
      "step 4400/5157 - loss: 0.0142 - Perplexity: 1.0079 - 195ms/step\n",
      "step 4600/5157 - loss: 0.0090 - Perplexity: 1.0079 - 195ms/step\n",
      "step 4800/5157 - loss: 0.0025 - Perplexity: 1.0080 - 195ms/step\n",
      "step 5000/5157 - loss: 1.5657e-04 - Perplexity: 1.0080 - 195ms/step\n",
      "step 5157/5157 - loss: 2.4874e-04 - Perplexity: 1.0079 - 195ms/step\n",
      "Epoch 4/4\n",
      "step  200/5157 - loss: 6.6991e-04 - Perplexity: 1.0052 - 194ms/step\n",
      "step  400/5157 - loss: 0.0113 - Perplexity: 1.0051 - 195ms/step\n",
      "step  600/5157 - loss: 4.2536e-04 - Perplexity: 1.0050 - 195ms/step\n",
      "step  800/5157 - loss: 4.5449e-04 - Perplexity: 1.0046 - 195ms/step\n",
      "step 1000/5157 - loss: 0.0021 - Perplexity: 1.0047 - 193ms/step\n",
      "step 1200/5157 - loss: 0.0069 - Perplexity: 1.0047 - 194ms/step\n",
      "step 1400/5157 - loss: 0.0020 - Perplexity: 1.0047 - 194ms/step\n",
      "step 1600/5157 - loss: 0.0010 - Perplexity: 1.0048 - 195ms/step\n",
      "step 1800/5157 - loss: 4.1455e-04 - Perplexity: 1.0048 - 196ms/step\n",
      "step 2000/5157 - loss: 5.6010e-04 - Perplexity: 1.0048 - 196ms/step\n",
      "step 2200/5157 - loss: 0.0050 - Perplexity: 1.0049 - 195ms/step\n",
      "step 2400/5157 - loss: 7.6859e-04 - Perplexity: 1.0050 - 195ms/step\n",
      "step 2600/5157 - loss: 7.0383e-04 - Perplexity: 1.0051 - 195ms/step\n",
      "step 2800/5157 - loss: 0.0386 - Perplexity: 1.0051 - 195ms/step\n",
      "step 3000/5157 - loss: 0.0027 - Perplexity: 1.0051 - 195ms/step\n",
      "step 3200/5157 - loss: 0.0325 - Perplexity: 1.0052 - 196ms/step\n",
      "step 3400/5157 - loss: 0.0022 - Perplexity: 1.0052 - 195ms/step\n",
      "step 3600/5157 - loss: 4.1617e-04 - Perplexity: 1.0053 - 195ms/step\n",
      "step 3800/5157 - loss: 0.0011 - Perplexity: 1.0053 - 195ms/step\n",
      "step 4000/5157 - loss: 6.0488e-04 - Perplexity: 1.0054 - 195ms/step\n",
      "step 4200/5157 - loss: 0.0026 - Perplexity: 1.0055 - 195ms/step\n",
      "step 4400/5157 - loss: 0.0012 - Perplexity: 1.0055 - 195ms/step\n",
      "step 4600/5157 - loss: 0.0243 - Perplexity: 1.0055 - 195ms/step\n",
      "step 4800/5157 - loss: 0.0072 - Perplexity: 1.0056 - 195ms/step\n",
      "step 5000/5157 - loss: 0.0058 - Perplexity: 1.0056 - 195ms/step\n",
      "step 5157/5157 - loss: 1.5340e-04 - Perplexity: 1.0056 - 195ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints_cotemfw/final\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataloader, batch_size=batch_size, epochs=epochs, save_freq=1000, save_dir='./checkpoints_cotemfw', log_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-20 14:33:43,764] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlenlp/data/vocab.py:217: UserWarning: The type of `to_tokens()`'s input `indices` is not `int` which will be forcibly transfered to `int`. \n",
      "  \"The type of `to_tokens()`'s input `indices` is not `int` which will be forcibly transfered to `int`. \"\n"
     ]
    }
   ],
   "source": [
    "# 导入预训练模型\n",
    "checkpoint_path = './checkpoints_cotemfw/final'  # 填写预训练模型的保存路径\n",
    "\n",
    "model = SkepForTokenClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=3)\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\n",
    "model = paddle.Model(model, [input])\n",
    "model.load(checkpoint_path)\n",
    "\n",
    "# 导入测试集\n",
    "test_dataloader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_len, for_test=True)\n",
    "# 预测保存\n",
    "\n",
    "save_file = {'cotebd': './submission/COTE_BD.tsv', 'cotedp': './submission/COTE_DP.tsv', 'cotemfw': './submission/COTE_MFW.tsv'}\n",
    "predicts = []\n",
    "input_ids = []\n",
    "for batch in test_dataloader:\n",
    "    predict = model.predict_batch(batch)\n",
    "    predicts += predict[0].argmax(axis=-1).tolist()\n",
    "    input_ids += batch.numpy().tolist()\n",
    "\n",
    "# 先找到B所在的位置，即标号为0的位置，然后顺着该位置一直找到所有的I，即标号为1，即为所得。\n",
    "def find_entity(prediction, input_ids):\n",
    "    entity = []\n",
    "    entity_ids = []\n",
    "    for index, idx in enumerate(prediction):\n",
    "        if idx == label_list['B']:\n",
    "            entity_ids = [input_ids[index]]\n",
    "        elif idx == label_list['I']:\n",
    "            if entity_ids:\n",
    "                entity_ids.append(input_ids[index])\n",
    "        elif idx == label_list['O']:\n",
    "            if entity_ids:\n",
    "                entity.append(''.join(tokenizer.convert_ids_to_tokens(entity_ids)))\n",
    "                entity_ids = []\n",
    "    return entity\n",
    "\n",
    "with open(save_file[data_name], 'w', encoding='utf8') as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for idx, sample in enumerate(data_dict[data_name]['test']):\n",
    "        qid = sample.split('\\t')[0]\n",
    "        entity = find_entity(predicts[idx], input_ids[idx])\n",
    "        entity = list(set(entity))  # 去重\n",
    "        f.write(qid + '\\t' + '\\x01'.join(entity) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: submission/ (stored 0%)\r\n",
      "  adding: submission/SE-ABSA16_PHNS.tsv (deflated 64%)\r\n",
      "  adding: submission/COTE_MFW.tsv (deflated 54%)\r\n",
      "  adding: submission/SE-ABSA16_CAME.tsv (deflated 66%)\r\n",
      "  adding: submission/COTE_BD.tsv (deflated 44%)\r\n",
      "  adding: submission/COTE_DP.tsv (deflated 54%)\r\n",
      "  adding: submission/NLPCC14-SC.tsv (deflated 64%)\r\n",
      "  adding: submission/ChnSentiCorp.tsv (deflated 63%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r submission.zip ./submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
